name: Transcribe Audio with Whisper

on:
  push:
    paths:
      - 'audio/**'
  workflow_dispatch:
    inputs:
      audio_path:
        description: 'Path to audio file (relative to repo root)'
        required: false
        default: 'audio/sample.mp3'
      model:
        description: 'Whisper model to use (tiny, base, small, medium, large)'
        required: false
        default: 'base'

jobs:
  transcribe:
    runs-on: ubuntu-latest
    steps:
      - name: Check out code
        uses: actions/checkout@v3

      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install system deps & Whisper
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg
          python -m pip install --upgrade pip
          pip install git+https://github.com/openai/whisper.git

      - name: Set model variable
        id: set_model
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "model=${{ github.event.inputs.model }}" >> $GITHUB_OUTPUT
          else
            echo "model=base" >> $GITHUB_OUTPUT
          fi

      - name: Transcribe audio
        run: |
          mkdir -p transcripts
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            whisper "${{ github.event.inputs.audio_path }}" \
              --model ${{ steps.set_model.outputs.model }} \
              --output_dir transcripts
          else
            for f in audio/*.{mp3,wav,m4a,flac}; do
              if [ -f "$f" ]; then
                whisper "$f" --model ${{ steps.set_model.outputs.model }} --output_dir transcripts
              fi
            done
          fi

      - name: Upload transcripts
        uses: actions/upload-artifact@v4
        with:
          name: whisper-transcripts
          path: transcripts/
